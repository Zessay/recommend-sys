{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/criteo/criteo_data.csv\")\n",
    "\n",
    "# 取出连续型特征和类别型特征对应的列\n",
    "con = [f for f in data.columns if f.startswith(\"I\")]\n",
    "cat = [f for f in data.columns if f.startswith(\"C\")]\n",
    "\n",
    "field_handler = FieldHandler(train_file_path=\"../data/criteo/criteo_data.csv\",\n",
    "                            continuation_columns=con,\n",
    "                            category_columns=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:  C1\n",
      "cat:  C2\n",
      "cat:  C3\n",
      "cat:  C4\n",
      "cat:  C5\n",
      "cat:  C6\n",
      "cat:  C7\n",
      "cat:  C8\n",
      "cat:  C9\n",
      "cat:  C10\n",
      "cat:  C11\n",
      "cat:  C12\n",
      "cat:  C13\n",
      "cat:  C14\n",
      "cat:  C15\n",
      "cat:  C16\n",
      "cat:  C17\n",
      "cat:  C18\n",
      "cat:  C19\n",
      "cat:  C20\n",
      "cat:  C21\n",
      "cat:  C22\n",
      "cat:  C23\n",
      "cat:  C24\n",
      "cat:  C25\n",
      "cat:  C26\n",
      "con:  I1\n",
      "con:  I2\n",
      "con:  I3\n",
      "con:  I4\n",
      "con:  I5\n",
      "con:  I6\n",
      "con:  I7\n",
      "con:  I8\n",
      "con:  I9\n",
      "con:  I10\n",
      "con:  I11\n",
      "con:  I12\n",
      "con:  I13\n"
     ]
    }
   ],
   "source": [
    "# 获取要输入的特征和标签值\n",
    "features, labels = transformation_data(data, \n",
    "                                      field_handler=field_handler,\n",
    "                                      label=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基础参数的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础参数设置\n",
    "class Config(dict):\n",
    "    def __init__(self, field_handler):\n",
    "        # 模型参数\n",
    "        self['field_size'] = len(field_handler.field_dict)\n",
    "        self['feature_size'] = field_handler.feature_nums\n",
    "        self['embedding_size'] = 50 \n",
    "        self['dropout_prob'] = [0.8, 0.8, 0.8]\n",
    "        self['attention_size'] = 10\n",
    "        self['seed'] = 2019 \n",
    "        self['l2_reg'] = 0.001\n",
    "        \n",
    "        # 训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['batch_size'] = 128 \n",
    "        self['evaluateEvery'] = 1000\n",
    "        self['checkpointEvery'] = 1000\n",
    "        self['lr'] = 0.01 \n",
    "        self['decay_steps'] = 200 \n",
    "        self['decay_rate'] = 0.9 \n",
    "        self['grad_clip'] = 5.0 \n",
    "        \n",
    "        # 其他参数\n",
    "        self['num_classes'] = 1 \n",
    "        self['train_size'] = 0.8 \n",
    "        self.threshold = 0.5 \n",
    "        self['checkpoint_dir'] = \"../model/AFM/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/AFM/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFM(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        tf.set_random_seed(self.config['seed'])\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.feat_index = tf.placeholder(tf.int32, shape=[None, self.config['field_size']], name=\"feat_index\")\n",
    "        self.feat_value = tf.placeholder(tf.float32, shape=[None, self.config['field_size']], name=\"feat_value\")\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, self.config['num_classes']], name=\"labels\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_prob\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        \n",
    "        # Embedding层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'], self.feat_index)\n",
    "            feat_value = tf.expand_dims(self.feat_value, 2)\n",
    "            ## 输出shape: [batch, field_size, embed_size]\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "        \n",
    "        # Attention层\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            ## 对每个field和其他field进行element-wise\n",
    "            element_wise_product_list = []\n",
    "            for i in range(self.config['field_size']):\n",
    "                for j in range(i+1, self.config['field_size']):\n",
    "                    element_wise_product_list.append(tf.multiply(self.embeddings[:, i, :], self.embeddings[:, j, :]))\n",
    "            ## 输出shape: [(f*(f-1))/2, batch, embed_size]\n",
    "            self.element_wise_product = tf.stack(element_wise_product_list)\n",
    "            ## ## 输出shape: [ batch, (f*(f-1))/2, embed_size]\n",
    "            self.element_wise_product = tf.transpose(self.element_wise_product, perm=[1, 0, 2],\n",
    "                                                         name=\"element_wise_product\")\n",
    "            \n",
    "            num_interactions = int(self.config[\"field_size\"] * (self.config[\"field_size\"] - 1) / 2)\n",
    "            \n",
    "            ## 转换到attention_size上\n",
    "            self.attention_wx_plus_b = tf.reshape(tf.add(tf.matmul(tf.reshape(self.element_wise_product, \n",
    "                                                                              [-1, self.config['embedding_size']]),\n",
    "                                                                  self.weights['attention_w']),\n",
    "                                                        self.weights['attention_b']),\n",
    "                                                 shape=[-1, num_interactions, self.config['attention_size']])\n",
    "            ## 输出shape: [batch, num_inter]\n",
    "            self.attention_activation = tf.reduce_sum(tf.multiply(tf.nn.relu(self.attention_wx_plus_b),\n",
    "                                                                 self.weights['attention_h']), axis=2)\n",
    "            \n",
    "            ## 归一化，得到每个交叉特征的权重\n",
    "            self.attention_alpha = tf.nn.softmax(self.attention_activation)\n",
    "            ## 对上面得到的二阶交叉特征进行加权\n",
    "            ## 输出shape: [batch, embed_size]\n",
    "            self.attention_x_product = tf.reduce_sum(tf.multiply(tf.expand_dims(self.attention_alpha, 2), self.element_wise_product), axis=1, name=\"afm\")\n",
    "            self.attention_x_product = tf.nn.dropout(self.attention_x_product, self.dropout_keep_prob[0])\n",
    "            \n",
    "            self.attention_part_sum = tf.matmul(self.attention_x_product,\n",
    "                                               self.weights['attention_p'])\n",
    "            \n",
    "        # 一阶特征\n",
    "        with tf.name_scope(\"first_order\"):\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights['feature_weights'], self.feat_index)\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)\n",
    "            \n",
    "            ## 最终的偏置\n",
    "            self.y_bias = self.weights['bias'] * tf.ones_like(self.labels)\n",
    "        \n",
    "        # 输出层\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.add_n([self.attention_part_sum, tf.reduce_sum(self.y_first_order, axis=1, keepdims=True), self.y_bias])\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "            \n",
    "        # 损失函数层\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2_reg'] > 0: \n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables()\n",
    "                                   if \"bia\" not in cand_var.name and \"embedding\" not in cand_var.name])\n",
    "                self.loss += self.config['l2_reg'] * l2_loss\n",
    "                \n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['lr'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            \n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "            \n",
    "            \n",
    "    def _init_weights(self):\n",
    "        weights = dict()\n",
    "        \n",
    "        # 设置embeddings参数\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([self.config['feature_size'], self.config['embedding_size']], 0.0, 0.01),\n",
    "                name=\"feature_embeddings\")\n",
    "        weights[\"feature_weights\"] = tf.Variable(tf.random_normal([self.config['feature_size'], 1], 0.0, 1.0),\n",
    "                                                name=\"feature_weights\")\n",
    "        weights['bias'] = tf.Variable(tf.constant(0.1), name=\"bias\")\n",
    "        \n",
    "        # Attention部分\n",
    "        glorot = np.sqrt(2.0 / (self.config['attention_size']+self.config['embedding_size']))\n",
    "        weights['attention_w'] = tf.Variable(np.random.normal(loc=0, scale=glorot,\n",
    "                                                             size=(self.config['embedding_size'], self.config['attention_size'])),\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             name=\"attention_w\")\n",
    "        weights['attention_b'] = tf.Variable(np.random.normal(loc=0, scale=glorot,\n",
    "                                                              size=(self.config['attention_size'])),\n",
    "                                            dtype=tf.float32, name=\"attention_b\")\n",
    "        weights['attention_h'] = tf.Variable(np.random.normal(loc=0, scale=1, \n",
    "                                                              size=(self.config['attention_size'])),\n",
    "                                            dtype=tf.float32, name=\"attention_h\")                                     \n",
    "        weights['attention_p'] = tf.Variable(np.ones((self.config['embedding_size'], 1)),\n",
    "                                            dtype=tf.float32)\n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        # 定义迭代次数\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc, train_f_score = metrics['accuracy'], metrics['f_score']\n",
    "            \n",
    "            ## 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer='train', scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0:\n",
    "                print(\"Train - Step: {} | Loss: {} | Acc: {} | F1_score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval)\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                acc_mean = np.round(metrics['accuracy'], 5)\n",
    "                gini_mean = np.round(metrics['gini_norm'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Eval | Loss: {} | Accuracy: {} | Gini: {}\".format(\n",
    "                    loss_mean, acc_mean, gini_mean))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"accuracy\":np.array(acc_mean),\n",
    "                                 \"gini\": np.array(gini_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0:\n",
    "                self.model.save(self.sess)\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_feat_i, batch_feat_v, batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.feat_index: batch_feat_i, \n",
    "                    self.model.feat_value: batch_feat_v, \n",
    "                    self.model.labels: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropout_prob'],\n",
    "                    self.model.is_training: True}\n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss, \n",
    "                                                   self.model.predictions,\n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step \n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        feed_dict = {self.model.feat_index: batch[0],\n",
    "                    self.model.feat_value: batch[1],\n",
    "                    self.model.labels: batch[2],\n",
    "                    self.model.dropout_keep_prob: [1.0] * len(self.config['dropout_prob']),\n",
    "                    self.model.is_training: False}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_idx = slice(0, int(len(labels)*0.8))\n",
    "val_idx = slice(int(len(labels)*0.8), int(len(labels)))\n",
    "\n",
    "train_df_i, train_df_v, train_df_y = (features[\"df_i\"][train_idx], \n",
    "                                      features[\"df_v\"][train_idx], \n",
    "                                      labels[train_idx])\n",
    "val_df_i, val_df_v, val_df_y = (features[\"df_i\"][val_idx],\n",
    "                               features[\"df_v\"][val_idx],\n",
    "                               labels[val_idx])\n",
    "\n",
    "train = DataGenerator(train_df_y, train_df_i, train_df_v)\n",
    "val = DataGenerator(val_df_y, val_df_i, val_df_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(field_handler)\n",
    "    config['num_epochs'] = 2 \n",
    "    create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "    tf.reset_default_graph()\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    \n",
    "    model = AFM(config)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    pack_data = [train, val]\n",
    "    logger = Logger(sess, config)\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b399dd3870c34d60a17d2b9439569b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 1000 | Loss: 0.5081796050071716 | Acc: 0.80315 | F1_score: 0.4898\n",
      "Eval | Loss: 0.5459700226783752 | Accuracy: 0.77094 | Gini: 0.48676\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 2000 | Loss: 0.48729002475738525 | Acc: 0.77165 | F1_score: 0.38298\n",
      "Eval | Loss: 0.5188999772071838 | Accuracy: 0.77526 | Gini: 0.5088\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 3000 | Loss: 0.46591147780418396 | Acc: 0.82677 | F1_score: 0.54167\n",
      "Eval | Loss: 0.5061200261116028 | Accuracy: 0.77588 | Gini: 0.51464\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 4000 | Loss: 0.4764001965522766 | Acc: 0.77953 | F1_score: 0.41667\n",
      "Eval | Loss: 0.4976100027561188 | Accuracy: 0.77785 | Gini: 0.52122\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 5000 | Loss: 0.4682236313819885 | Acc: 0.7874 | F1_score: 0.52632\n",
      "Eval | Loss: 0.4929400086402893 | Accuracy: 0.77794 | Gini: 0.52438\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 6000 | Loss: 0.3941747546195984 | Acc: 0.83465 | F1_score: 0.55319\n",
      "Eval | Loss: 0.4897400140762329 | Accuracy: 0.77874 | Gini: 0.52769\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3dbc32d4444c6e8385e9117d394cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 7000 | Loss: 0.48639893531799316 | Acc: 0.76378 | F1_score: 0.375\n",
      "Eval | Loss: 0.49160000681877136 | Accuracy: 0.77895 | Gini: 0.52886\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 8000 | Loss: 0.4770505428314209 | Acc: 0.80315 | F1_score: 0.46809\n",
      "Eval | Loss: 0.49171000719070435 | Accuracy: 0.77922 | Gini: 0.52886\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 9000 | Loss: 0.42882412672042847 | Acc: 0.8189 | F1_score: 0.54902\n",
      "Eval | Loss: 0.48583999276161194 | Accuracy: 0.77949 | Gini: 0.53109\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 10000 | Loss: 0.4669159948825836 | Acc: 0.77953 | F1_score: 0.41667\n",
      "Eval | Loss: 0.48695001006126404 | Accuracy: 0.7795 | Gini: 0.53087\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 11000 | Loss: 0.4677627980709076 | Acc: 0.7874 | F1_score: 0.4\n",
      "Eval | Loss: 0.4878099858760834 | Accuracy: 0.77954 | Gini: 0.53078\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 12000 | Loss: 0.41931575536727905 | Acc: 0.85039 | F1_score: 0.61224\n",
      "Eval | Loss: 0.4857800006866455 | Accuracy: 0.77957 | Gini: 0.53106\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
