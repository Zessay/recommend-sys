{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:13.384966Z",
     "start_time": "2019-08-22T01:57:01.285831Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:18.195404Z",
     "start_time": "2019-08-22T01:57:16.041292Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:19.099348Z",
     "start_time": "2019-08-22T01:57:19.090780Z"
    }
   },
   "outputs": [],
   "source": [
    "class FieldHandler(object):\n",
    "    def __init__(self, train_file_path, test_file_path=None, category_columns=[], continuation_columns=[]):\n",
    "        '''\n",
    "        :param train_file_path: 训练集文件文件名\n",
    "        :param test_file_path: 测试集文件文件名\n",
    "        :param category_columns: 类别型特征, list型\n",
    "        :param continuation_columns: 连续型特征, list型\n",
    "        '''\n",
    "        self.train_file_path = None\n",
    "        self.test_file_path = None\n",
    "        self.feature_nums = 0   ## 不预留，不同field的nan值单独处理\n",
    "        self.field_dict = {}\n",
    "\n",
    "\n",
    "        self.category_columns = category_columns\n",
    "        self.continuation_columns = continuation_columns\n",
    "\n",
    "        if not isinstance(train_file_path, str):\n",
    "            raise ValueError(\"train file path must str\")\n",
    "        if os.path.exists(train_file_path):\n",
    "            self.train_file_path = train_file_path\n",
    "        else:\n",
    "            raise OSError(\"train file path isn't exists!\")\n",
    "\n",
    "        if test_file_path:\n",
    "            if os.path.exists(test_file_path):\n",
    "                self.test_file_path = test_file_path\n",
    "            else:\n",
    "                raise OSError(\"test file path isn't exists!\")\n",
    "        ## 读取数据\n",
    "        self.read_data()\n",
    "\n",
    "        ## 构建场到特征的字典\n",
    "        self.build_field_dict()\n",
    "        self.build_standard_scaler()\n",
    "        ## 该模型只对类别型特征进行embedding\n",
    "        self.field_nums = len(self.category_columns)\n",
    "        self.num_feats = len(self.continuation_columns)\n",
    "\n",
    "    def read_data(self):\n",
    "        '''\n",
    "        读取数据\n",
    "        '''\n",
    "        if self.train_file_path and self.test_file_path:\n",
    "            train_df = pd.read_csv(self.train_file_path)[self.category_columns + self.continuation_columns]\n",
    "            test_df = pd.read_csv(self.test_file_path)[self.category_columns + self.continuation_columns]\n",
    "            self.df = pd.concat([train_df, test_df])\n",
    "        else:\n",
    "            self.df = pd.read_csv(self.train_file_path)[self.category_columns + self.continuation_columns]\n",
    "            \n",
    "        self.df[self.category_columns] = self.df[self.category_columns].astype(str)\n",
    "\n",
    "\n",
    "    def build_field_dict(self):\n",
    "        '''\n",
    "        构建场到特征的映射关系\n",
    "        '''\n",
    "        for column in self.df.columns:\n",
    "            if column in self.category_columns:\n",
    "                ## 类别型特征中所有不同的值\n",
    "                ## 不特殊对待缺失值\n",
    "                cv = [f for f in self.df[column].unique()]\n",
    "                ## 将每一种特征值对应到不同的特征标号，键表示对应的场\n",
    "                self.field_dict[column] = dict(zip(cv, range(self.feature_nums, self.feature_nums+len(cv))))\n",
    "                ## 对应特征数增加\n",
    "                self.feature_nums += len(cv)\n",
    "\n",
    "    def build_standard_scaler(self):\n",
    "        '''\n",
    "        对连续型特征进行标准化\n",
    "        '''\n",
    "        if self.continuation_columns:\n",
    "            self.standard_scaler = StandardScaler()\n",
    "            self.standard_scaler.fit(self.df[self.continuation_columns].values)\n",
    "        else:\n",
    "            self.standard_scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:19.490224Z",
     "start_time": "2019-08-22T01:57:19.482674Z"
    }
   },
   "outputs": [],
   "source": [
    "def transformation_data(file_path, field_hander, label=None):\n",
    "    '''\n",
    "    返回准备好的数据\n",
    "    :param label: 目标值对应的列名\n",
    "    :return:\n",
    "    '''\n",
    "    df_ = pd.read_csv(file_path)\n",
    "    if label:\n",
    "        if label in df_.columns:\n",
    "            ## 获取对应的目标值\n",
    "            labels = df_[[label]].values.astype(\"float32\")\n",
    "        else:\n",
    "            raise KeyError(f\"label '{label}' isn\\'t exists!\")\n",
    "    df_v = df_[field_hander.category_columns]\n",
    "    num_features = df_[field_hander.continuation_columns]\n",
    "    \n",
    "    del df_ \n",
    "    gc.collect()\n",
    "    \n",
    "    ## 对连续型特征和类别型特征的缺失值进行填充\n",
    "    ## 对连续型特征进行归一化\n",
    "    if field_hander.standard_scaler:\n",
    "        num_features[field_hander.continuation_columns] = field_hander.standard_scaler.transform(num_features.values)\n",
    "    ## 对连续型特征的缺失值进行填充\n",
    "    num_features.fillna(0, inplace=True)\n",
    "\n",
    "    ## 这个DataFrame用于记录每个类别型特征值和连续型特征对应的特征标号\n",
    "    df_i = df_v.copy()\n",
    "\n",
    "    for column in df_v.columns:\n",
    "        print(\"cat: \", column)\n",
    "        df_i[column] = df_i[column].map(field_hander.field_dict[column])\n",
    "        ## 对于测试集，可能有的特征值没有在训练集中出现\n",
    "        ## 第0号特征留给缺失值\n",
    "        ##df_i[column].fillna(0, inplace=True)\n",
    "        ## 对非缺失值赋值为1\n",
    "        df_v[column] = 1\n",
    "        ## 对值序列的缺失值用0填充\n",
    "        ##df_v[column].fillna(0, inplace=True)\n",
    "\n",
    "    cat_v = df_v.values.astype(\"float32\")\n",
    "    cat_i = df_i.values.astype(\"int32\")\n",
    "    num_features = num_features.values.astype(\"float32\")\n",
    "    features = {\n",
    "        \"cat_i\": cat_i,\n",
    "        \"cat_v\": cat_v,\n",
    "        \"num_feats\": num_features\n",
    "    }\n",
    "\n",
    "    if label:\n",
    "        return features, labels\n",
    "    return features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:25.148274Z",
     "start_time": "2019-08-22T01:57:19.948236Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/criteo/criteo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:25.151789Z",
     "start_time": "2019-08-22T01:57:25.149567Z"
    }
   },
   "outputs": [],
   "source": [
    "## 取出连续型特征和类别型特征\n",
    "con = [f for f in data.columns if f.startswith(\"I\")]\n",
    "cat = [f for f in data.columns if f.startswith(\"C\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:57:31.454319Z",
     "start_time": "2019-08-22T01:57:25.641352Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义fieldhandler对象\n",
    "field_handler = FieldHandler(train_file_path=\"../data/criteo/criteo_data.csv\", continuation_columns=con,\n",
    "                           category_columns=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:00.492156Z",
     "start_time": "2019-08-22T01:57:32.011667Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:  C1\n",
      "cat:  C2\n",
      "cat:  C3\n",
      "cat:  C4\n",
      "cat:  C5\n",
      "cat:  C6\n",
      "cat:  C7\n",
      "cat:  C8\n",
      "cat:  C9\n",
      "cat:  C10\n",
      "cat:  C11\n",
      "cat:  C12\n",
      "cat:  C13\n",
      "cat:  C14\n",
      "cat:  C15\n",
      "cat:  C16\n",
      "cat:  C17\n",
      "cat:  C18\n",
      "cat:  C19\n",
      "cat:  C20\n",
      "cat:  C21\n",
      "cat:  C22\n",
      "cat:  C23\n",
      "cat:  C24\n",
      "cat:  C25\n",
      "cat:  C26\n"
     ]
    }
   ],
   "source": [
    "# 获取要输入的特征和标签值\n",
    "features, labels = transformation_data(file_path=\"../data/criteo/criteo_data.csv\",\n",
    "                                      field_hander=field_handler, label=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义基础配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:00.498249Z",
     "start_time": "2019-08-22T01:58:00.493358Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, field_handler):\n",
    "        self['seed'] = 2019 \n",
    "\n",
    "        self['deep_layers'] = [32, 32]\n",
    "        self['cross_layer_num'] = 3\n",
    "        self['deep_layers_activation'] = \"relu\"\n",
    "        self['dropout'] = [0.5, 0.5, 0.5]\n",
    "        self['l2_reg'] = 0.05 \n",
    "        self['lr'] = 0.01 \n",
    "        self['max_to_keep'] = 5\n",
    "        self['batch_size'] = 128 \n",
    "        self['num_epochs'] = 5 \n",
    "        self.threshold = 0.5 \n",
    "        self.eval = 1000 \n",
    "        self.checkpoint = 1000 \n",
    "         \n",
    "        \n",
    "        self['cat_feat_size'] = field_handler.feature_nums\n",
    "        self['embedding_size'] = 5\n",
    "        self['field_size'] = field_handler.field_nums\n",
    "        self['num_feat_size'] = field_handler.num_feats\n",
    "        self['total_size'] = self['field_size']*self['embedding_size'] + self['num_feat_size']\n",
    "        \n",
    "        self['checkpoint_dir'] = \"../model/DCN/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/DCN/summary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T02:03:56.641875Z",
     "start_time": "2019-08-22T02:03:56.612008Z"
    }
   },
   "outputs": [],
   "source": [
    "class DCN(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        tf.set_random_seed(self.config[\"seed\"])\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # 定义输入\n",
    "        ## 类别型特征\n",
    "        self.feat_index = tf.placeholder(tf.int32, shape=[None, None], name=\"feat_index\")\n",
    "        self.feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"feat_value\")\n",
    "        ## 数值型特征\n",
    "        self.numeric_value = tf.placeholder(tf.float32, [None, None], name=\"num_value\")\n",
    "        ## 标签值\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, 1], name=\"label\")\n",
    "        ## dropout\n",
    "        self.dropout_keep_deep = tf.placeholder(tf.float32, shape=[None], name=\"dropout_deep\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "\n",
    "\n",
    "        self.weights = self._initialize_weights()\n",
    "\n",
    "        ######################### Embedding ################################\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            ## 输出 [batch, field_size, embedding_size]\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embedding'], self.feat_index)\n",
    "            ## 形状 [batch, field_size, 1]\n",
    "            feat_value = tf.expand_dims(self.feat_value, axis=2)\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "\n",
    "        # 将数值型特征和嵌入特征拼接 [batch, total_size]\n",
    "        self.x0 = tf.concat([self.numeric_value, tf.reshape(self.embeddings, \n",
    "                                                            shape=[-1, self.config['field_size']*self.config['embedding_size']])],\n",
    "                           axis=1)\n",
    "\n",
    "        ######################## Deep NetWork ###############################\n",
    "\n",
    "        self.deep_layers_activation = None\n",
    "        if self.config['deep_layers_activation'] == \"relu\":\n",
    "            self.deep_layers_activation = tf.nn.relu\n",
    "        elif self.config['deep_layers_activation'] == \"tanh\":\n",
    "            self.deep_layers_activation = tf.nn.tanh\n",
    "        elif self.config['deep_layers_activation'] == \"sigmoid\":\n",
    "            self.deep_layers_activation = tf.nn.sigmoid\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"DeepLayer\"):\n",
    "            self.y_deep = tf.nn.dropout(self.x0, self.dropout_keep_deep[0])\n",
    "\n",
    "            ## 最终输出 [batch, deep_layers[-1]]\n",
    "            for i in range(0, len(self.config[\"deep_layers\"])):\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[f\"deep_layer_{i}\"]),\n",
    "                                    self.weights[f\"deep_bias_{i}\"])\n",
    "                self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_deep[i+1])\n",
    "\n",
    "        ######################### Cross NetWork ################################\n",
    "\n",
    "        with tf.name_scope(\"CrossLayer\"):\n",
    "            ## 维度: [batch, total_size, 1]\n",
    "            self._x0 = tf.expand_dims(self.x0, axis=2)\n",
    "            x_l = self._x0\n",
    "            for l in range(self.config['cross_layer_num']):\n",
    "                '''\n",
    "                x_l = tf.tensordot(tf.matmul(self._x0, x_l, transpose_b=True),\n",
    "                                  self.weights[f\"cross_layer_{l}\"], 1) + self.weights[f\"cross_bias_{l}\"] + x_l\n",
    "                这种计算方法会消耗比较大的空间，下面是优化方法\n",
    "                '''\n",
    "                ## 这里只会得到标量\n",
    "                x_b = tf.tensordot(tf.transpose(x_l, [0, 2, 1]), self.weights[f\"cross_layer_{l}\"], 1)\n",
    "                x_l = tf.multiply(self._x0, x_b) + self.weights[f\"cross_bias_{l}\"] + x_l\n",
    "                \n",
    "                \n",
    "            self.cross_network_out = tf.squeeze(x_l, axis=2)\n",
    "\n",
    "        ######################### Output Layer ##################################\n",
    "        with tf.name_scope(\"Output\"):\n",
    "            concat_input = tf.concat([self.cross_network_out, self.y_deep], axis=1)\n",
    "            self.logits = tf.add(tf.matmul(concat_input, self.weights['concat_projection']), \n",
    "                                self.weights['concat_bias'])\n",
    "\n",
    "        ############################### Loss ###################################\n",
    "        self.l2_loss = 0\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "            losses = tf.losses.log_loss(self.labels, self.predictions)\n",
    "            '''\n",
    "            l2_regular = tf.contrib.layers.l2_regularizer(scale=self.config['l2_reg'])\n",
    "\n",
    "            keys = [w for w in self.weights.keys() if \"bias\" not in w and \"embedding\" not in w]\n",
    "            for key in keys:\n",
    "                self.l2_loss += tf.contrib.layers.apply_regularization(l2_regular,\n",
    "                                                                      [self.weights[key]])\n",
    "            '''\n",
    "            if self.config['l2_reg'] > 0: \n",
    "                self.l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables() \n",
    "                                    if \"bia\" and \"embedding\" not in cand_var.name])\n",
    "            \n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) + self.config['l2_reg'] * self.l2_loss\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.AdamOptimizer(self.config['lr'])\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=self.global_step_tensor)\n",
    "                \n",
    "    \n",
    "    # 初始化权重\n",
    "    def _initialize_weights(self):\n",
    "        weights = dict()\n",
    "        ## embedding层权重\n",
    "        weights[\"feature_embedding\"] = tf.Variable(\n",
    "            tf.random_normal([self.config['cat_feat_size'], self.config[\"embedding_size\"]],\n",
    "                            0.0, 0.01), name=\"cat_embeddings\")\n",
    "        weights[\"feature_bias\"] = tf.Variable(\n",
    "            tf.random_normal([self.config['cat_feat_size'], 1], 0.0, 1.0), name=\"cat_bias\")\n",
    "        \n",
    "        ## 深层网络的权重\n",
    "        num_layers = len(self.config['deep_layers'])  ## 表示网络的数量\n",
    "        ### 第一层的权重特殊处理\n",
    "        glorot = np.sqrt(2.0/(self.config['total_size']+self.config['deep_layers'][0]))\n",
    "        weights['deep_layer_0'] = tf.Variable(\n",
    "            tf.random_normal(shape=[self.config['total_size'], self.config['deep_layers'][0]],\n",
    "                            mean=0.0, stddev=glorot, dtype=tf.float32), name=\"deep_layer_0\")\n",
    "        weights['deep_bias_0'] = tf.Variable(tf.constant(0.0, shape=[self.config['deep_layers'][0]]), \n",
    "                                             name=\"deep_bias_0\")\n",
    "        \n",
    "        ### 其它层权重\n",
    "        for i in range(1, num_layers):\n",
    "            glorot = np.sqrt(2.0 / (self.config['deep_layers'][i-1]+self.config['deep_layers'][i]))\n",
    "            weights[f\"deep_layer_{i}\"] = tf.Variable(\n",
    "                tf.random_normal(shape=[self.config['deep_layers'][i-1], self.config['deep_layers'][i]],\n",
    "                                mean=0.0, stddev=glorot, dtype=tf.float32), name=f\"deep_layer_{i}\")\n",
    "            weights[f\"deep_bias_{i}\"] = tf.Variable(tf.constant(0.0, shape=[self.config['deep_layers'][i]]),\n",
    "                                                   name=f\"deep_bias_{i}\")\n",
    "        \n",
    "        ## 交叉层\n",
    "        for i in range(self.config[\"cross_layer_num\"]):\n",
    "            glorot = np.sqrt(2.0/(self.config['total_size']+1))\n",
    "            weights[f\"cross_layer_{i}\"] = tf.Variable(\n",
    "                tf.random_normal(shape=[self.config['total_size'], 1], mean=0, stddev=glorot,\n",
    "                                dtype=tf.float32), name=f\"cross_layer_{i}\")\n",
    "            weights[f\"cross_bias_{i}\"] = tf.Variable(tf.constant(0.0), name=f\"cross_bias_{i}\")\n",
    "        \n",
    "        \n",
    "        ## 最后一层投影层\n",
    "        input_size = self.config['total_size'] + self.config['deep_layers'][-1]\n",
    "        glorot = np.sqrt(2.0/(input_size+1))\n",
    "        weights['concat_projection'] = tf.Variable(\n",
    "            tf.random_normal(shape=[input_size, 1], dtype=tf.float32), name=\"concat_projection\")\n",
    "        weights['concat_bias'] = tf.Variable(tf.constant(0.01), dtype=tf.float32)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config[\"max_to_keep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:25.838929Z",
     "start_time": "2019-08-22T01:58:25.814025Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        # 定义迭代次数\n",
    "        num_iter_per_epoch = self.train.length // self.config[\"batch_size\"]\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc, train_f_score = metrics[\"accuracy\"], metrics[\"f_score\"]\n",
    "            \n",
    "            ## 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss,\n",
    "                             \"acc\": np.array(train_acc), \n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer=\"train\", scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config.eval == 0: \n",
    "                print(\"Train - Step: {} | Loss: {} | Acc: {} | F1_Score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config[\"batch_size\"]):\n",
    "                    loss, predictions = self.eval_step(batchEval[0], batchEval[1], batchEval[2], batchEval[3])\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                acc_mean = np.round(metrics[\"accuracy\"], 5)\n",
    "                gini_mean = np.round(metrics[\"gini_norm\"], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Eval | Loss: {} | Accuracy: {} | Gini: {}\".format(\n",
    "                    loss_mean, acc_mean, gini_mean))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean), \n",
    "                                 \"accuracy\": np.array(acc_mean), \n",
    "                                 \"gini\": np.array(gini_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config.checkpoint == 0: \n",
    "                self.model.save(self.sess)\n",
    "        \n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_feat_i, batch_feat_v, batch_num_feats, batch_y = next(self.train.next_batch(self.config[\"batch_size\"]))\n",
    "        \n",
    "        feed_dict = {self.model.feat_index: batch_feat_i, self.model.feat_value: batch_feat_v,\n",
    "                    self.model.numeric_value: batch_num_feats, \n",
    "                    self.model.labels: batch_y,\n",
    "                    self.model.dropout_keep_deep: self.config['dropout'],\n",
    "                    self.model.is_training: True}\n",
    "        \n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss,\n",
    "                                                   self.model.predictions, \n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        \n",
    "        return loss, metrics, step\n",
    "    \n",
    "    def eval_step(self, *batch):\n",
    "        feed_dict = {self.model.feat_index: batch[0], self.model.feat_value: batch[1],\n",
    "                    self.model.numeric_value: batch[2],\n",
    "                    self.model.labels: batch[3],\n",
    "                    self.model.dropout_keep_deep: [1.0]*len(self.config['dropout']),\n",
    "                    self.model.is_training: False}\n",
    "        \n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据包装类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:26.732626Z",
     "start_time": "2019-08-22T01:58:26.727986Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, labels, *features):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.length = len(labels)\n",
    "        ## 计算不同类别的比例\n",
    "        unique = Counter(self.labels.ravel())\n",
    "        self.ratio = [(key, value / self.length) for key, value in unique.items()]\n",
    "        self.indices = []\n",
    "        for key, _ in self.ratio:\n",
    "            index = np.where(labels.ravel() == key)\n",
    "            self.indices.append(index)\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        '''\n",
    "        生成每一个batch的数据集\n",
    "        '''\n",
    "        choose = np.array([])\n",
    "        for i in range(len(self.indices)):\n",
    "            ## 按照在数据集中出现的比例采样\n",
    "            idx = np.random.choice(self.indices[i][0],\n",
    "                                   max(1, min(len(self.indices[i][0]), int(batch_size * self.ratio[i][1]))))\n",
    "            '''\n",
    "            ## 等比例采样\n",
    "            idx = np.random.choice(self.indices[i][0],\n",
    "                                  min(len(self.indices[i][0]), int(batch_size / len(self.indices))))\n",
    "            '''\n",
    "            choose = np.append(choose, idx)\n",
    "        choose = np.random.permutation(choose).astype(\"int64\")\n",
    "        result = []\n",
    "        for feat in self.features:\n",
    "            result.append(feat[choose])\n",
    "        result.append(labels[choose])\n",
    "        yield result\n",
    "        \n",
    "    def iter_all(self, batch_size):\n",
    "        '''\n",
    "        按照batch迭代所有数据\n",
    "        '''\n",
    "        numBatches = self.length // batch_size + 1 \n",
    "        for i in range(numBatches):\n",
    "            result = []\n",
    "            start = i*batch_size\n",
    "            end = min(start+batch_size, self.length)\n",
    "            for feat in self.features:\n",
    "                result.append(np.asarray(feat[start:end]))\n",
    "            result.append(np.asarray(self.labels[start:end]))\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:28.380325Z",
     "start_time": "2019-08-22T01:58:28.377059Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_idx = slice(0, int(len(labels)*0.8))\n",
    "val_idx = slice(int(len(labels)*0.8), int(len(labels)))\n",
    "\n",
    "train_cat_i, train_cat_v, train_num_feats, train_df_y = (features[\"cat_i\"][train_idx], \n",
    "                                                         features[\"cat_v\"][train_idx], \n",
    "                                                         features[\"num_feats\"][train_idx],\n",
    "                                                        labels[train_idx])\n",
    "\n",
    "val_cat_i, val_cat_v, val_num_feats, val_df_y = (features[\"cat_i\"][val_idx],\n",
    "                                                features[\"cat_v\"][val_idx],\n",
    "                                                features[\"num_feats\"][val_idx],\n",
    "                                                labels[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:28.829278Z",
     "start_time": "2019-08-22T01:58:28.827424Z"
    }
   },
   "outputs": [],
   "source": [
    "train = DataGenerator(train_df_y, train_cat_i, train_cat_v, train_num_feats)\n",
    "val = DataGenerator(val_df_y, val_cat_i, val_cat_v, val_num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T01:58:30.355380Z",
     "start_time": "2019-08-22T01:58:30.352799Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dirs(dirs):\n",
    "    try:\n",
    "        for dir_ in dirs: \n",
    "            if not os.path.exists(dir_):\n",
    "                os.makedirs(dir_)\n",
    "        return 0 \n",
    "    except Exception as e:\n",
    "        print(\"Creating directories error: {}\".format(e))\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T02:04:01.380264Z",
     "start_time": "2019-08-22T02:04:01.360548Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(field_handler)\n",
    "    config[\"num_epochs\"] = 2 \n",
    "    create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "    tf.reset_default_graph()\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "    session_conf.gpu_options.allow_growth=True \n",
    "    \n",
    "    model = DCN(config)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    pack_data = [train, val]\n",
    "    logger = Logger(sess, config)\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T02:05:34.013994Z",
     "start_time": "2019-08-22T02:04:01.539774Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6578f4cee94fcaa141e19606cd3209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 1000 | Loss: 0.6166831254959106 | Acc: 0.73228 | F1_Score: 0.37037\n",
      "Eval | Loss: 0.5381799936294556 | Accuracy: 0.7614 | Gini: 0.46184\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 2000 | Loss: 0.5165849924087524 | Acc: 0.75591 | F1_Score: 0.43636\n",
      "Eval | Loss: 0.5215799808502197 | Accuracy: 0.76585 | Gini: 0.48042\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 3000 | Loss: 0.5001413226127625 | Acc: 0.77953 | F1_Score: 0.41667\n",
      "Eval | Loss: 0.5181900262832642 | Accuracy: 0.76751 | Gini: 0.48972\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 4000 | Loss: 0.42506060004234314 | Acc: 0.85039 | F1_Score: 0.64151\n",
      "Eval | Loss: 0.5306100249290466 | Accuracy: 0.76836 | Gini: 0.4776\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 5000 | Loss: 0.4473000168800354 | Acc: 0.80315 | F1_Score: 0.46809\n",
      "Eval | Loss: 0.5252400040626526 | Accuracy: 0.76806 | Gini: 0.47944\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 6000 | Loss: 0.4930572509765625 | Acc: 0.77953 | F1_Score: 0.41667\n",
      "Eval | Loss: 0.5246300101280212 | Accuracy: 0.76556 | Gini: 0.48186\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f165ab1ef347aeb8cc9e05a235368b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 7000 | Loss: 0.4515524208545685 | Acc: 0.81102 | F1_Score: 0.53846\n",
      "Eval | Loss: 0.5466200113296509 | Accuracy: 0.75428 | Gini: 0.46097\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 8000 | Loss: 0.45302814245224 | Acc: 0.84252 | F1_Score: 0.54545\n",
      "Eval | Loss: 0.5668799877166748 | Accuracy: 0.75434 | Gini: 0.43185\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 9000 | Loss: 0.6132492423057556 | Acc: 0.81102 | F1_Score: 0.58621\n",
      "Eval | Loss: 0.678380012512207 | Accuracy: 0.75132 | Gini: 0.42468\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 10000 | Loss: 0.5281270146369934 | Acc: 0.85827 | F1_Score: 0.64\n",
      "Eval | Loss: 0.6008700132369995 | Accuracy: 0.75483 | Gini: 0.40901\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 11000 | Loss: 0.41974037885665894 | Acc: 0.80315 | F1_Score: 0.44444\n",
      "Eval | Loss: 0.5525100231170654 | Accuracy: 0.76112 | Gini: 0.43948\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 12000 | Loss: 0.442089706659317 | Acc: 0.81102 | F1_Score: 0.57143\n",
      "Eval | Loss: 0.5473600029945374 | Accuracy: 0.76196 | Gini: 0.44675\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-18-ea40edb50d22>:49: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:514: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fc1bde662e452782fc6c28772be1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 1000 | Loss: 0.5230885744094849 | Acc: 0.76378 | F1_Score: 0.25\n",
      "Eval | Loss: 0.5120499730110168 | Accuracy: 0.77171 | Gini: 0.48846\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 2000 | Loss: 0.5670076012611389 | Acc: 0.77165 | F1_Score: 0.32558\n",
      "Eval | Loss: 0.5231500267982483 | Accuracy: 0.76834 | Gini: 0.47225\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 3000 | Loss: 0.45411279797554016 | Acc: 0.79528 | F1_Score: 0.45833\n",
      "Eval | Loss: 0.5318800210952759 | Accuracy: 0.76844 | Gini: 0.47352\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 4000 | Loss: 0.4850400686264038 | Acc: 0.80315 | F1_Score: 0.46809\n",
      "Eval | Loss: 0.528219997882843 | Accuracy: 0.76277 | Gini: 0.46865\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 5000 | Loss: 0.5219179391860962 | Acc: 0.77953 | F1_Score: 0.44\n",
      "Eval | Loss: 0.5261300206184387 | Accuracy: 0.76669 | Gini: 0.4809\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 6000 | Loss: 0.49063974618911743 | Acc: 0.79528 | F1_Score: 0.48\n",
      "Eval | Loss: 0.5372800230979919 | Accuracy: 0.76016 | Gini: 0.45578\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c6b9eb93446aeaefa8f610d8de072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 7000 | Loss: 0.4394581913948059 | Acc: 0.80315 | F1_Score: 0.46809\n",
      "Eval | Loss: 0.5674300193786621 | Accuracy: 0.7615 | Gini: 0.44235\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 8000 | Loss: 0.4340980052947998 | Acc: 0.82677 | F1_Score: 0.57692\n",
      "Eval | Loss: 0.5304800271987915 | Accuracy: 0.76287 | Gini: 0.45965\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 9000 | Loss: 0.38260096311569214 | Acc: 0.86614 | F1_Score: 0.67925\n",
      "Eval | Loss: 0.5470799803733826 | Accuracy: 0.76702 | Gini: 0.4629\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 10000 | Loss: 0.34689629077911377 | Acc: 0.85827 | F1_Score: 0.67857\n",
      "Eval | Loss: 0.5615400075912476 | Accuracy: 0.75629 | Gini: 0.45107\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 11000 | Loss: 0.37602999806404114 | Acc: 0.85039 | F1_Score: 0.65455\n",
      "Eval | Loss: 0.5833899974822998 | Accuracy: 0.75693 | Gini: 0.4091\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 12000 | Loss: 0.35748952627182007 | Acc: 0.83465 | F1_Score: 0.61818\n",
      "Eval | Loss: 0.5859400033950806 | Accuracy: 0.75872 | Gini: 0.44062\n",
      "Saving model...\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 不对nan值进行特殊处理，直接当做一个特征\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
