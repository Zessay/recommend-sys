{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import os \n",
    "from collections import Counter\n",
    "from tqdm.autonotebook import tqdm \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基本配置和数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 基本配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self):\n",
    "        self['data_dir'] = \"../data/ml-1m/ratings.dat\"\n",
    "        self['data_path'] = \"../data/ml_lm\"\n",
    "        self['column_names'] = ['user', 'item']\n",
    "        \n",
    "        # 用于训练数据生成的参数\n",
    "        self['num_neg'] = 10    ## 表示每个user生成的负样本的数量\n",
    "        self['test_neg'] = 99  ## 测试集每个user的负样本数量\n",
    "        self['batch_size'] = 256    ## 表示每个batch的大小\n",
    "        self['topk'] = 10 \n",
    "        self['seed'] = 2019\n",
    "        \n",
    "        # 模型参数\n",
    "        self['embed_size'] = 20  # 表示嵌入向量\n",
    "        self['dropout_prob'] = 0.8 \n",
    "        self['l2_reg'] = 0.001\n",
    "        \n",
    "        # 训练参数\n",
    "        self['num_epochs'] = 2 \n",
    "        self['evaluateEvery'] = 1000\n",
    "        self['checkpointEvery'] = 1000\n",
    "        self['lr'] = 0.01 \n",
    "        self['decay_steps'] = 200 \n",
    "        self['decay_rate'] = 0.9 \n",
    "        self['grad_clip'] = 5.0 \n",
    "        \n",
    "        # 其他参数\n",
    "        self['num_classes'] = 1 \n",
    "        self['train_size'] = 0.8 \n",
    "        self.threshold = 0.5 \n",
    "        self['checkpoint_dir'] = \"../model/NCF/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/NCF/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.load_data()\n",
    "    \n",
    "\n",
    "    def load_data(self):\n",
    "        full_data = pd.read_csv(self.config['data_dir'], sep=\"::\", header=None, names=self.config['column_names'],\n",
    "                               usecols=[0,1], dtype={0:np.int32, 1:np.int32}, engine='python')\n",
    "        full_data[\"user\"] = full_data[\"user\"] - 1 \n",
    "        user_set = set(full_data['user'].unique())\n",
    "        item_set = set(full_data['item'].unique())\n",
    "        # 获取用户的数量和商品的数量\n",
    "        self.user_size = len(user_set)\n",
    "        self.item_size = len(item_set)\n",
    "        ## 对商品进行重新编号\n",
    "        item_map = self.re_index(item_set)\n",
    "        item_list = []\n",
    "        full_data['item'] = full_data['item'].map(lambda x: item_map[x])\n",
    "        item_set = set(full_data.item.unique())\n",
    "        ## 记录每个用户有过行为的商品\n",
    "        self.user_bought = {}\n",
    "        for i in range(len(full_data)):\n",
    "            u = full_data['user'][i]\n",
    "            t = full_data['item'][i]\n",
    "            if u not in self.user_bought:\n",
    "                self.user_bought[u] = []\n",
    "            self.user_bought[u].append(t)\n",
    "        \n",
    "        ## 对于每个用户来说的负样本\n",
    "        self.user_negative = {}\n",
    "        for key in self.user_bought:\n",
    "            self.user_negative[key] = list(item_set - set(self.user_bought[key]))\n",
    "        \n",
    "        # 划分训练集和测试集\n",
    "        ## 按照用户进行分组，得到每个用户有过行为的商品的数量\n",
    "        user_length= full_data.groupby('user').size().tolist()\n",
    "        split_train_test = []\n",
    "        \n",
    "        ## 将每个用户前n-1个作为训练集，第n个作为测试集\n",
    "        for i in range(len(user_set)):\n",
    "            for _ in range(user_length[i]-1):\n",
    "                split_train_test.append(\"train\")\n",
    "            split_train_test.append(\"test\")\n",
    "            \n",
    "        full_data['split'] = split_train_test\n",
    "        \n",
    "        train_data = full_data[full_data['split'] == \"train\"].reset_index(drop=True)\n",
    "        test_data = full_data[full_data['split']==\"test\"].reset_index(drop=True)\n",
    "        del train_data['split']\n",
    "        del test_data['split']\n",
    "        \n",
    "        labels = np.ones(len(train_data), dtype=np.int32)\n",
    "        ## features是DataFrame结构，labels是ndarray结构\n",
    "        self.train_features = train_data\n",
    "        self.train_labels = labels\n",
    "        \n",
    "        self.test_features = test_data\n",
    "        self.test_labels = test_data['item'].values\n",
    "        \n",
    "    \n",
    "    def add_negative(self, features, labels, numbers, is_training):\n",
    "        feature_user, feature_item, labels_add, feature_dict = [],[],[],{}\n",
    "        for i in range(len(features)):\n",
    "            user = features['user'][i]\n",
    "            item = features['item'][i]\n",
    "            label = labels[i]\n",
    "            \n",
    "            feature_user.append(user)\n",
    "            feature_item.append(item)\n",
    "            labels_add.append(label)\n",
    "            \n",
    "            ## 添加负样本\n",
    "            neg_samples = np.random.choice(self.user_negative[user], size=numbers, replace=False)\n",
    "            \n",
    "            if is_training:\n",
    "                for k in neg_samples:\n",
    "                    feature_user.append(user)\n",
    "                    feature_item.append(k)\n",
    "                    labels_add.append(0)\n",
    "            else:\n",
    "                for k in neg_samples:\n",
    "                    feature_user.append(user)\n",
    "                    feature_item.append(k)\n",
    "                    labels_add.append(k)\n",
    "        feature_dict['user'] = feature_user\n",
    "        feature_dict['item'] = feature_item\n",
    "        \n",
    "        return feature_dict, labels_add\n",
    "    \n",
    "    def dump_data(self, features, labels,  num_neg, is_training):\n",
    "        if not os.path.exists(self.config['data_path']):\n",
    "            os.makedirs(self.config['data_path'])\n",
    "            \n",
    "        features, labels = self.add_negative(features,  labels, num_neg, is_training)\n",
    "        \n",
    "        data_dict=  dict([('user', features['user']), \n",
    "                         ('item', features['item']), \n",
    "                         ('label', labels)])\n",
    "        if is_training:\n",
    "            np.save(os.path.join(self.config['data_path'], \"train_data.npy\"), data_dict)\n",
    "        else:\n",
    "            np.save(os.path.join(self.config['data_path'], \"test_data.npy\"), data_dict)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def re_index(self, x):\n",
    "        i = 0\n",
    "        x_map = {}\n",
    "        for key in x:\n",
    "            x_map[key] = i\n",
    "            i += 1 \n",
    "        return x_map\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen:\n",
    "    def __init__(self, config, dataset):\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def train_input_fn(self):\n",
    "        '''构造训练数据'''\n",
    "        data_path = os.path.join(self.config['data_path'], 'train_data.npy')\n",
    "        # 如果不存在则生成数据文件\n",
    "        if not os.path.exists(data_path):\n",
    "            self.dataset.dump_data(self.dataset.train_features, self.dataset.train_labels, \n",
    "                                   self.config['num_neg'], True)\n",
    "        ## 加载数据\n",
    "        data = np.load(data_path, allow_pickle=True).item()\n",
    "        print(\"Loading train data finished!\")\n",
    "        datagen = tf.data.Dataset.from_tensor_slices(data)\n",
    "        datagen = datagen.shuffle(100000).batch(self.config['batch_size'])\n",
    "        return datagen\n",
    "    \n",
    "    def eval_input_fn(self):\n",
    "        '''构造测试数据'''\n",
    "        data_path = os.path.join(self.config['data_path'], 'test_data.npy')\n",
    "        # 如果不存在则生成文件\n",
    "        if not os.path.exists(data_path):\n",
    "            self.dataset.dump_data(self.dataset.test_features, self.dataset.test_labels, \n",
    "                           self.config['test_neg'], False)\n",
    "        data = np.load(data_path, allow_pickle=True).item()\n",
    "        print(\"Loading test data finished!\")\n",
    "        datagen = tf.data.Dataset.from_tensor_slices(data)\n",
    "        datagen = datagen.batch(self.config['test_neg']+1)\n",
    "        return datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(BaseModel):\n",
    "    def __init__(self, config, iterator):\n",
    "        super().__init__(config)\n",
    "        tf.set_random_seed(self.config['seed'])\n",
    "        self.initializer = tf.initializers.glorot_normal()\n",
    "        self.iterator = iterator\n",
    "        \n",
    "        self.get_data()\n",
    "        self.build_model()\n",
    "        self.evaluation()\n",
    "        self.init_saver()\n",
    "\n",
    "        \n",
    "    def get_data(self):\n",
    "        sample = self.iterator.get_next()\n",
    "        self.user = sample['user']\n",
    "        self.item = sample['item']\n",
    "        self.label = tf.cast(sample['label'], tf.float32)\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.name_scope(\"input\"):\n",
    "            self.user_onehot = tf.one_hot(self.user, self.config['user_size'], name=\"user_onehot\")\n",
    "            self.item_onehot = tf.one_hot(self.item, self.config['item_size'], name='item_onehot')\n",
    "            self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "            \n",
    "        with tf.name_scope(\"embed\"):\n",
    "            self.user_embed_GMF = tf.layers.dense(inputs=self.user_onehot,\n",
    "                                                 units=self.config['embed_size'],\n",
    "                                                 activation=tf.nn.relu,\n",
    "                                                 kernel_initializer=self.initializer,\n",
    "                                                 name=\"user_embed_GMF\")\n",
    "            self.item_embed_GMF = tf.layers.dense(inputs=self.item_onehot,\n",
    "                                                 units=self.config['embed_size'],\n",
    "                                                 activation=tf.nn.relu,\n",
    "                                                 kernel_initializer=self.initializer,\n",
    "                                                 name=\"item_embed_GMF\")\n",
    "            self.user_embed_MLP = tf.layers.dense(inputs=self.user_onehot,\n",
    "                                                 units=self.config['embed_size'],\n",
    "                                                 activation=tf.nn.relu,\n",
    "                                                 kernel_initializer=self.initializer,\n",
    "                                                 name=\"user_embed_MLP\")\n",
    "            self.item_embed_MLP = tf.layers.dense(inputs=self.item_onehot,\n",
    "                                                 units=self.config['embed_size'],\n",
    "                                                 kernel_initializer=self.initializer,\n",
    "                                                 name=\"item_embed_MLP\")\n",
    "            \n",
    "        with tf.name_scope(\"GMF\"):\n",
    "            # 将对应的user和item隐向量相乘\n",
    "            self.GMF = tf.multiply(self.user_embed_GMF, self.item_embed_GMF, name=\"GMF\")\n",
    "            \n",
    "        with tf.name_scope(\"MLP\"):\n",
    "            # 获取user和item的深层交互特征\n",
    "            self.interaction = tf.concat([self.user_embed_MLP, self.item_embed_MLP],\n",
    "                                        axis=-1, name='interaction')\n",
    "            self.layer1_MLP = tf.layers.dense(inputs=self.interaction,\n",
    "                                             units=self.config['embed_size']*2,\n",
    "                                             activation=tf.nn.relu,\n",
    "                                             kernel_initializer=self.initializer,\n",
    "                                             name='layer1_MLP')\n",
    "            self.layer1_MLP = tf.nn.dropout(self.layer1_MLP, self.dropout_keep_prob)\n",
    "            \n",
    "            self.layer2_MLP = tf.layers.dense(inputs=self.layer1_MLP,\n",
    "                                             units=self.config['embed_size'],\n",
    "                                             activation=tf.nn.relu,\n",
    "                                             kernel_initializer=self.initializer,\n",
    "                                             name=\"layer2_MLP\")\n",
    "            self.layer2_MLP = tf.nn.dropout(self.layer2_MLP, self.dropout_keep_prob)\n",
    "            \n",
    "            self.layer3_MLP = tf.layers.dense(inputs=self.layer2_MLP,\n",
    "                                             units=self.config['embed_size']//2,\n",
    "                                             activation=tf.nn.relu,\n",
    "                                             kernel_initializer=self.initializer,\n",
    "                                             name=\"layer3_MLP\")\n",
    "            self.layer3_MLP = tf.nn.dropout(self.layer3_MLP, self.dropout_keep_prob)\n",
    "            \n",
    "        with tf.name_scope('output'):\n",
    "            self.concatenation = tf.concat([self.GMF, self.layer3_MLP], axis=-1, name=\"concatenation\")\n",
    "            self.logits = tf.layers.dense(inputs=self.concatenation,\n",
    "                                         units=1,\n",
    "                                         activation=None, \n",
    "                                         kernel_initializer=self.initializer,\n",
    "                                         name=\"layer4_output\")\n",
    "            self.logits_dense = tf.reshape(self.logits, [-1])\n",
    "            self.predictions = tf.nn.sigmoid(self.logits_dense)\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.label,\n",
    "                                                                              logits=self.logits_dense))\n",
    "            #l2_loss = tf.constant(0.0)\n",
    "            if self.config['l2_reg'] > 0:\n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables() \n",
    "                                   if \"layer\" in cand_var.name])\n",
    "                self.loss += self.config['l2_reg'] * l2_loss\n",
    "                \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['lr'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            \n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "    \n",
    "    def evaluation(self):\n",
    "        with tf.name_scope(\"evaluation\"):\n",
    "            # 当数据是测试集的时候，所有的item属于同一个用户的\n",
    "            _, self.indice = tf.nn.top_k(self.predictions, self.config['topk'])\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 定义训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义metrics\n",
    "def mrr(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        ## 计算倒数，表示越靠前值越大\n",
    "        return np.reciprocal(float(index+1))\n",
    "    return 0\n",
    "    \n",
    "def hit(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        return 1 \n",
    "    return 0 \n",
    "\n",
    "def ndcg(gt_item, pred_items):\n",
    "    if gt_item in pred_items:\n",
    "        index = np.where(pred_items == gt_item)[0][0]\n",
    "        return np.reciprocal(np.log2(index+2))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.sess.run(self.model.iterator.make_initializer(self.train))\n",
    "        self.model.get_data()\n",
    "        start_time = time.time()\n",
    "        print(\"Train: \")\n",
    "        try:\n",
    "            while True:\n",
    "                loss, step = self.train_step()\n",
    "                summaries_dict = {\"loss\": loss}\n",
    "                self.logger.summarize(step, summarizer='train', scope='train_summary',\n",
    "                                     summaries_dict=summaries_dict)\n",
    "                if step % self.config['evaluateEvery'] == 0:\n",
    "                    print(f\"Step: {step}, Loss: {loss}\")\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Took: {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "\n",
    "        ## 用测试集对训练得到的模型进行评估\n",
    "        print(\"Evaluation: \")\n",
    "        self.sess.run(self.model.iterator.make_initializer(self.eval))\n",
    "        self.model.get_data()\n",
    "        start_time = time.time()\n",
    "        HR, MRR, NDCG = [], [], []\n",
    "        try:\n",
    "            while True:\n",
    "                prediction, labels = self.eval_step()\n",
    "                label = int(labels[0])\n",
    "                ## 计算各项指标\n",
    "                HR.append(hit(label, prediction))\n",
    "                MRR.append(mrr(label, prediction))\n",
    "                NDCG.append(ndcg(label, prediction))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            hr_mean = np.array(HR).mean()\n",
    "            mrr_mean = np.array(MRR).mean()\n",
    "            ndcg_mean = np.array(NDCG).mean()\n",
    "            print(\"Took: {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))))\n",
    "            print(\"HR is {:.3f}, MRR is {:.3f}, NDCG is {:.3f}\".format(hr_mean, mrr_mean, ndcg_mean))\n",
    "            summaries_dict = {\"HR\": hr_mean, \"MRR\": mrr_mean, \"NDCG\": ndcg_mean}\n",
    "            self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "        ## 保存模型\n",
    "        self.model.save(self.sess)\n",
    "        \n",
    "\n",
    "    def train_step(self):\n",
    "        feed_dict = {self.model.dropout_keep_prob: self.config['dropout_prob']}\n",
    "        _, loss, step = self.sess.run([self.model.train_op, self.model.loss, self.model.global_step_tensor],\n",
    "                                  feed_dict=feed_dict)\n",
    "        return loss, step\n",
    "        \n",
    "    def eval_step(self):\n",
    "        feed_dict = {self.model.dropout_keep_prob: 1.0}\n",
    "        indice, item = self.sess.run([self.model.indice, self.model.item],\n",
    "                                    feed_dict = feed_dict)\n",
    "        ## 按照预测排名列出商品\n",
    "        #print(np.unique(user))\n",
    "        prediction = np.take(item, indice)\n",
    "        return prediction, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tf.reset_default_graph()\n",
    "    config = Config()\n",
    "    dataset = Dataset(config)\n",
    "    data = DataGen(config, dataset)\n",
    "    train_data = data.train_input_fn()\n",
    "    test_data = data.eval_input_fn()\n",
    "    \n",
    "    config['user_size'] = dataset.user_size\n",
    "    config['item_size'] = dataset.item_size\n",
    "    create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "    model = NCF(config, iterator)\n",
    "\n",
    "    print(\"加载已经存在的模型...\")\n",
    "    model.load(sess)\n",
    "    \n",
    "    pack_data = [train_data, test_data]\n",
    "    logger = Logger(sess, config)\n",
    "\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data finished!\n",
      "Loading test data finished!\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-7-fee63f91c870>:31: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-fee63f91c870>:60: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "加载已经存在的模型...\n",
      "\n",
      "当前正处于第1次迭代\n",
      "Train: \n",
      "Step: 1000, Loss: 0.37952202558517456\n",
      "Step: 2000, Loss: 0.38519033789634705\n",
      "Step: 3000, Loss: 0.3475574254989624\n",
      "Step: 4000, Loss: 0.36829236149787903\n",
      "Step: 5000, Loss: 0.39247995615005493\n",
      "Step: 6000, Loss: 0.4150899648666382\n",
      "Step: 7000, Loss: 0.30751311779022217\n",
      "Step: 8000, Loss: 0.41413211822509766\n",
      "Step: 9000, Loss: 0.3936382532119751\n",
      "Step: 10000, Loss: 0.33736452460289\n",
      "Step: 11000, Loss: 0.39479273557662964\n",
      "Step: 12000, Loss: 0.35984179377555847\n",
      "Step: 13000, Loss: 0.3317415714263916\n",
      "Step: 14000, Loss: 0.3633555471897125\n",
      "Step: 15000, Loss: 0.4040967524051666\n",
      "Step: 16000, Loss: 0.37653374671936035\n",
      "Step: 17000, Loss: 0.33586615324020386\n",
      "Step: 18000, Loss: 0.35446324944496155\n",
      "Step: 19000, Loss: 0.3694964051246643\n",
      "Took: 00:02:07\n",
      "Evaluation: \n",
      "Took: 00:00:04\n",
      "HR is 0.403, MRR is 0.208, NDCG is 0.253\n",
      "Saving model...\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n",
      "Train: \n",
      "Step: 20000, Loss: 0.3698214888572693\n",
      "Step: 21000, Loss: 0.35963353514671326\n",
      "Step: 22000, Loss: 0.32726168632507324\n",
      "Step: 23000, Loss: 0.34930557012557983\n",
      "Step: 24000, Loss: 0.36546728014945984\n",
      "Step: 25000, Loss: 0.40116775035858154\n",
      "Step: 26000, Loss: 0.34393686056137085\n",
      "Step: 27000, Loss: 0.3531465232372284\n",
      "Step: 28000, Loss: 0.391345351934433\n",
      "Step: 29000, Loss: 0.27735647559165955\n",
      "Step: 30000, Loss: 0.3683692514896393\n",
      "Step: 31000, Loss: 0.32911086082458496\n",
      "Step: 32000, Loss: 0.35632994771003723\n",
      "Step: 33000, Loss: 0.3552132546901703\n",
      "Step: 34000, Loss: 0.40033701062202454\n",
      "Step: 35000, Loss: 0.37696459889411926\n",
      "Step: 36000, Loss: 0.38956767320632935\n",
      "Step: 37000, Loss: 0.34683042764663696\n",
      "Step: 38000, Loss: 0.327758252620697\n",
      "Took: 00:01:55\n",
      "Evaluation: \n",
      "Took: 00:00:04\n",
      "HR is 0.420, MRR is 0.228, NDCG is 0.272\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
