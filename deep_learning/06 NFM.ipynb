{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/criteo/criteo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出连续型特征和类别型特征对应的列\n",
    "con = [f for f in data.columns if f.startswith(\"I\")]\n",
    "cat = [f for f in data.columns if f.startswith(\"C\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_handler = FieldHandler(train_file_path=\"../data/criteo/criteo_data.csv\",\n",
    "                            continuation_columns=con,\n",
    "                            category_columns=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:  C1\n",
      "cat:  C2\n",
      "cat:  C3\n",
      "cat:  C4\n",
      "cat:  C5\n",
      "cat:  C6\n",
      "cat:  C7\n",
      "cat:  C8\n",
      "cat:  C9\n",
      "cat:  C10\n",
      "cat:  C11\n",
      "cat:  C12\n",
      "cat:  C13\n",
      "cat:  C14\n",
      "cat:  C15\n",
      "cat:  C16\n",
      "cat:  C17\n",
      "cat:  C18\n",
      "cat:  C19\n",
      "cat:  C20\n",
      "cat:  C21\n",
      "cat:  C22\n",
      "cat:  C23\n",
      "cat:  C24\n",
      "cat:  C25\n",
      "cat:  C26\n",
      "con:  I1\n",
      "con:  I2\n",
      "con:  I3\n",
      "con:  I4\n",
      "con:  I5\n",
      "con:  I6\n",
      "con:  I7\n",
      "con:  I8\n",
      "con:  I9\n",
      "con:  I10\n",
      "con:  I11\n",
      "con:  I12\n",
      "con:  I13\n"
     ]
    }
   ],
   "source": [
    "# 获取要输入的特征和标签值\n",
    "features, labels = transformation_data(data, \n",
    "                                      field_handler=field_handler,\n",
    "                                      label=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础参数设置\n",
    "class Config(dict):\n",
    "    def __init__(self, field_handler):\n",
    "        # 模型参数\n",
    "        self['field_size'] = len(field_handler.field_dict)\n",
    "        self['feature_size'] = field_handler.feature_nums\n",
    "        self['embedding_size'] = 50 \n",
    "        self['dropout_prob'] = [0.8, 0.8, 0.8]\n",
    "        self['deep_layers'] = [64, 64, 64]\n",
    "        self['seed'] = 2019 \n",
    "        self['l2_reg'] = 0.001\n",
    "        \n",
    "        # 训练参数\n",
    "        self['num_epochs'] = 5 \n",
    "        self['batch_size'] = 128 \n",
    "        self['evaluateEvery'] = 1000\n",
    "        self['checkpointEvery'] = 1000\n",
    "        self['lr'] = 0.01 \n",
    "        self['decay_steps'] = 200 \n",
    "        self['decay_rate'] = 0.9 \n",
    "        self['grad_clip'] = 5.0 \n",
    "        \n",
    "        # 其他参数\n",
    "        self['num_classes'] = 1 \n",
    "        self['train_size'] = 0.8 \n",
    "        self.threshold = 0.5 \n",
    "        self['checkpoint_dir'] = \"../model/NFM/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/NFM/summary\"\n",
    "        self['max_to_keep'] = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFM(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        tf.set_random_seed(self.config['seed'])\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "        \n",
    "    def build_model(self):\n",
    "        # 输入占位符\n",
    "        self.feat_index = tf.placeholder(tf.int32, shape=[None, None], name=\"feat_index\")\n",
    "        self.feat_value = tf.placeholder(tf.float32, shape=[None, None], name=\"feat_value\")\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, self.config['num_classes']], name=\"label\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name=\"dropout_keep_prob\")\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        \n",
    "        # embedding层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            ## 输出shape: [batch, feat_size, embed_size]\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],\n",
    "                                                    self.feat_index)\n",
    "            feat_value = tf.expand_dims(self.feat_value, 2)\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "            \n",
    "        # 一阶的结果\n",
    "        with tf.name_scope(\"first-order\"):\n",
    "            self.y_first_order = tf.nn.embedding_lookup(self.weights['feature_weights'], self.feat_index)\n",
    "            ## 输出shape: [batch, feat_size]\n",
    "            self.y_first_order = tf.reduce_sum(tf.multiply(self.y_first_order, feat_value), 2)\n",
    "        \n",
    "        # 二阶交叉结果\n",
    "        with tf.name_scope(\"second-order\"):\n",
    "            ## sum_square_part: [batch, feat_size]\n",
    "            self.summed_features_emb = tf.reduce_sum(self.embeddings, 1)\n",
    "            self.summed_features_emb_square = tf.square(self.summed_features_emb)\n",
    "            \n",
    "            ## square_sum_part: [batch, feat_size]\n",
    "            self.squared_features_emb = tf.square(self.embeddings)\n",
    "            self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, 1)\n",
    "            \n",
    "            self.y_second_order = 0.5 * tf.subtract(self.summed_features_emb_square, self.squared_sum_features_emb)\n",
    "        \n",
    "        # Deep层\n",
    "        with tf.name_scope(\"deep-layer\"):\n",
    "            self.y_deep = self.y_second_order\n",
    "            for i in range(len(self.config['deep_layers'])):\n",
    "                self.y_deep = tf.layers.dense(self.y_deep, self.config['deep_layers'][i],\n",
    "                                             kernel_initializer=tf.initializers.glorot_normal(),\n",
    "                                             bias_initializer=tf.initializers.constant(0.1),\n",
    "                                             activation=None,\n",
    "                                             name=f\"dense_{i}\")\n",
    "                self.y_deep = tf.nn.relu(self.y_deep)\n",
    "                self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_prob[i])\n",
    "                \n",
    "        \n",
    "        # 输出层\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.y_bias = self.weights['bias'] * tf.ones_like(self.labels)\n",
    "            self.logits = tf.add_n([tf.reduce_sum(self.y_first_order, axis=1, keepdims=True),\n",
    "                                   tf.reduce_sum(self.y_deep, axis=1, keepdims=True),\n",
    "                                   self.y_bias])\n",
    "            self.predictions = tf.nn.sigmoid(self.logits)\n",
    "            \n",
    "        # 损失函数\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels,\n",
    "                                                            logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            if self.config['l2_reg'] > 0: \n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) \n",
    "                                    for cand_var in tf.trainable_variables() \n",
    "                                    if \"bia\" not in cand_var.name and \"embedding\" not in cand_var.name])\n",
    "                self.loss += self.config['l2_reg'] * l2_loss\n",
    "                \n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            learning_rate = tf.train.exponential_decay(self.config['lr'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            # 使用梯度削减防止梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def _init_weights(self):\n",
    "        weights = dict()\n",
    "        \n",
    "        # embeddings初始化\n",
    "        weights['feature_embeddings'] = tf.Variable(tf.truncated_normal([self.config['feature_size'], self.config['embedding_size']],\n",
    "                                                                       0.0, 0.01), name=\"feature_embeddings\")\n",
    "        weights['feature_weights'] = tf.Variable(tf.truncated_normal([self.config['feature_size'], 1], 0.0, 1.0),\n",
    "                                             name=\"feature_weights\")\n",
    "        weights['bias'] = tf.Variable(tf.constant(0.1), name=\"bias\")\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        # 定义迭代次数\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc, train_f_score = metrics['accuracy'], metrics['f_score']\n",
    "            \n",
    "            ## 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer='train', scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0:\n",
    "                print(\"Train - Step: {} | Loss: {} | Acc: {} | F1_score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval)\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                acc_mean = np.round(metrics['accuracy'], 5)\n",
    "                gini_mean = np.round(metrics['gini_norm'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Eval | Loss: {} | Accuracy: {} | Gini: {}\".format(\n",
    "                    loss_mean, acc_mean, gini_mean))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"accuracy\":np.array(acc_mean),\n",
    "                                 \"gini\": np.array(gini_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0:\n",
    "                self.model.save(self.sess)\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_feat_i, batch_feat_v, batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.feat_index: batch_feat_i, \n",
    "                    self.model.feat_value: batch_feat_v, \n",
    "                    self.model.labels: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropout_prob'],\n",
    "                    self.model.is_training: True}\n",
    "        _, loss, predictions, step = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss, \n",
    "                                                   self.model.predictions,\n",
    "                                                   self.model.global_step_tensor],\n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step \n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        feed_dict = {self.model.feat_index: batch[0],\n",
    "                    self.model.feat_value: batch[1],\n",
    "                    self.model.labels: batch[2],\n",
    "                    self.model.dropout_keep_prob: [1.0] * len(self.config['dropout_prob']),\n",
    "                    self.model.is_training: False}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_idx = slice(0, int(len(labels)*0.8))\n",
    "val_idx = slice(int(len(labels)*0.8), int(len(labels)))\n",
    "\n",
    "train_df_i, train_df_v, train_df_y = (features[\"df_i\"][train_idx], \n",
    "                                      features[\"df_v\"][train_idx], \n",
    "                                      labels[train_idx])\n",
    "val_df_i, val_df_v, val_df_y = (features[\"df_i\"][val_idx],\n",
    "                               features[\"df_v\"][val_idx],\n",
    "                               labels[val_idx])\n",
    "\n",
    "train = DataGenerator(train_df_y, train_df_i, train_df_v)\n",
    "val = DataGenerator(val_df_y, val_df_i, val_df_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(field_handler)\n",
    "    config['num_epochs'] = 2 \n",
    "    create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "    tf.reset_default_graph()\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    \n",
    "    model = NFM(config)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    pack_data = [train, val]\n",
    "    logger = Logger(sess, config)\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-9-59e0aa86588c>:52: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-59e0aa86588c>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eae07765f1d48d3b8fd59c9063d872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 1000 | Loss: 0.5596783757209778 | Acc: 0.74016 | F1_score: 0.29787\n",
      "Eval | Loss: 0.5529299974441528 | Accuracy: 0.76815 | Gini: 0.47415\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 2000 | Loss: 0.5200712084770203 | Acc: 0.75591 | F1_score: 0.2439\n",
      "Eval | Loss: 0.5337799787521362 | Accuracy: 0.77094 | Gini: 0.48206\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 3000 | Loss: 0.5162172317504883 | Acc: 0.77165 | F1_score: 0.38298\n",
      "Eval | Loss: 0.5192099809646606 | Accuracy: 0.7702 | Gini: 0.4905\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 4000 | Loss: 0.49115726351737976 | Acc: 0.75591 | F1_score: 0.27907\n",
      "Eval | Loss: 0.5057899951934814 | Accuracy: 0.77305 | Gini: 0.50249\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 5000 | Loss: 0.479180246591568 | Acc: 0.7874 | F1_score: 0.4\n",
      "Eval | Loss: 0.49764999747276306 | Accuracy: 0.77358 | Gini: 0.50992\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 6000 | Loss: 0.45700758695602417 | Acc: 0.7874 | F1_score: 0.4\n",
      "Eval | Loss: 0.49605000019073486 | Accuracy: 0.77336 | Gini: 0.51016\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711586f23f514387afd03d0234715a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 7000 | Loss: 0.42785683274269104 | Acc: 0.8189 | F1_score: 0.54902\n",
      "Eval | Loss: 0.49608999490737915 | Accuracy: 0.77178 | Gini: 0.50899\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 8000 | Loss: 0.46074774861335754 | Acc: 0.75591 | F1_score: 0.27907\n",
      "Eval | Loss: 0.4953700006008148 | Accuracy: 0.77224 | Gini: 0.50954\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 9000 | Loss: 0.5141562223434448 | Acc: 0.75591 | F1_score: 0.47458\n",
      "Eval | Loss: 0.4950000047683716 | Accuracy: 0.77134 | Gini: 0.50958\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 10000 | Loss: 0.47839486598968506 | Acc: 0.77165 | F1_score: 0.50847\n",
      "Eval | Loss: 0.49480000138282776 | Accuracy: 0.77135 | Gini: 0.5085\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 11000 | Loss: 0.4579806625843048 | Acc: 0.80315 | F1_score: 0.5283\n",
      "Eval | Loss: 0.4945699870586395 | Accuracy: 0.77152 | Gini: 0.50829\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 12000 | Loss: 0.4137086570262909 | Acc: 0.85039 | F1_score: 0.64151\n",
      "Eval | Loss: 0.49441999197006226 | Accuracy: 0.77164 | Gini: 0.50878\n",
      "Saving model...\n",
      "Model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
