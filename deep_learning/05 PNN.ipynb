{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T05:18:52.440528Z",
     "start_time": "2019-08-26T05:18:50.090132Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 数据的准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/criteo/criteo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出连续型特征和类别型特征\n",
    "con = [f for f in data.columns if f.startswith(\"I\")]\n",
    "cat = [f for f in data.columns if f.startswith(\"C\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_handler = FieldHandler(train_file_path=\"../data/criteo/criteo_data.csv\",\n",
    "                            continuation_columns=con,\n",
    "                            category_columns=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat:  C1\n",
      "cat:  C2\n",
      "cat:  C3\n",
      "cat:  C4\n",
      "cat:  C5\n",
      "cat:  C6\n",
      "cat:  C7\n",
      "cat:  C8\n",
      "cat:  C9\n",
      "cat:  C10\n",
      "cat:  C11\n",
      "cat:  C12\n",
      "cat:  C13\n",
      "cat:  C14\n",
      "cat:  C15\n",
      "cat:  C16\n",
      "cat:  C17\n",
      "cat:  C18\n",
      "cat:  C19\n",
      "cat:  C20\n",
      "cat:  C21\n",
      "cat:  C22\n",
      "cat:  C23\n",
      "cat:  C24\n",
      "cat:  C25\n",
      "cat:  C26\n",
      "con:  I1\n",
      "con:  I2\n",
      "con:  I3\n",
      "con:  I4\n",
      "con:  I5\n",
      "con:  I6\n",
      "con:  I7\n",
      "con:  I8\n",
      "con:  I9\n",
      "con:  I10\n",
      "con:  I11\n",
      "con:  I12\n",
      "con:  I13\n"
     ]
    }
   ],
   "source": [
    "# 获取要输入的特征和标签值\n",
    "features, labels = transformation_data(data,\n",
    "                                      field_hander=field_handler,\n",
    "                                      label = \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, field_handler):\n",
    "        # 模型参数\n",
    "        self['field_size'] = len(field_handler.field_dict)\n",
    "        self['feature_size'] = field_handler.feature_nums\n",
    "        self['embedding_size'] = 50\n",
    "        self['deep_init_size'] = 100\n",
    "        self['deep_layers'] = [64, 64]\n",
    "        self['dropout_prob'] = [0.5, 0.8, 0.8] # 要比deep_layers多一个\n",
    "        self['seed'] = 2019\n",
    "        self['l2_reg'] = 0.001\n",
    "        self['use_inner'] = False\n",
    "        \n",
    "        # 训练参数\n",
    "        self['num_epochs'] = 5\n",
    "        self['batch_size'] = 128\n",
    "        self['evaluateEvery'] = 1000\n",
    "        self['checkpointEvery'] = 1000\n",
    "        self['lr'] = 0.01\n",
    "        self['decay_steps'] = 200 \n",
    "        self['decay_rate'] = 0.9\n",
    "        self['grad_clip'] = 5.0 \n",
    "        \n",
    "        # 其他参数\n",
    "        self['num_classes'] = 1 \n",
    "        self['train_size'] = 0.8 \n",
    "        self.threshold = 0.5\n",
    "        self['checkpoint_dir'] = \"../model/PNN/checkpoint\"\n",
    "        self['summary_dir'] = \"../model/PNN/summary\"\n",
    "        self['max_to_keep'] = 5\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNN(BaseModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        tf.set_random_seed(self.config['seed'])\n",
    "        self.build_model()\n",
    "        self.init_saver()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # 指定输入\n",
    "        self.feat_index = tf.placeholder(tf.int32, shape=[None, None], name='feat_index')\n",
    "        self.feat_value = tf.placeholder(tf.float32, shape=[None, None], name='feat_value')\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[None, self.config['num_classes']], name='label')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, shape=[None], name='dropout_keep_prob')\n",
    "        self.is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        \n",
    "        # Embedding层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            ## 输出 [batch, seq_len, embed_size]\n",
    "            self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'], self.feat_index)\n",
    "            feat_value = tf.reshape(self.feat_value, shape=[-1, self.config['field_size'], 1])\n",
    "            ## 输出 [batch, seq_len, embed_size]\n",
    "            self.embeddings = tf.multiply(self.embeddings, feat_value)\n",
    "            \n",
    "        # 线性层\n",
    "        with tf.name_scope(\"linear_singal\"):\n",
    "            ## 对计算速度进行优化\n",
    "            ### embeddings的维度为 [batch, seq_len, embed_size]， product的维度是[deep_size, seq_len, embed_size]\n",
    "            ### 输出为 [batch, deep_size]\n",
    "            self.lz = tf.tensordot(self.embeddings, self.weights['product_linear'], axes=((1, 2), (1, 2)), name=\"linear_dot\")\n",
    "\n",
    "            \n",
    "            \n",
    "        # 特征交叉层\n",
    "        with tf.name_scope(\"quardatic_singal\"):\n",
    "            if self.config['use_inner']:\n",
    "                ## 输出 [batch, embed_size, deep_size]\n",
    "                theta = tf.tensordot(self.embeddings, self.weights['product_quadratic_inner'], axes=((1), (1)), name=\"inner_dot\")\n",
    "                ## 输出 [batch, deep_size]\n",
    "                self.lp = tf.norm(theta, axis=1)\n",
    "            else:\n",
    "                embedding_sum = tf.reduce_sum(self.embeddings, axis=1)\n",
    "                ## 输出 [batch, embed_size, embed_size]\n",
    "                p = tf.matmul(tf.expand_dims(embedding_sum, 2), tf.expand_dims(embedding_sum, 1))\n",
    "                ## 输出 [batch, deep_size]\n",
    "                self.lp = tf.tensordot(p, self.weights['product_quadratic_outer'], axes=((1, 2), (1, 2)), name=\"outer_dot\")\n",
    "                \n",
    "        ## 将线性层和交叉层相加\n",
    "        self.y_deep = tf.nn.relu(tf.add(tf.add(self.lz, self.lp), self.weights['product_bias']))\n",
    "        self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_prob[0])\n",
    "        \n",
    "        # 深层网络\n",
    "        with tf.name_scope(\"deep_layers\"):\n",
    "            for i in range(0, len(self.config['deep_layers'])):\n",
    "                '''\n",
    "                self.y_deep = tf.add(tf.matmul(self.y_deep, self.weights[f\"layer_{i}\"]), self.weights[f'bias_{i}'], name=f\"deep_layer_{i}\")\n",
    "                '''\n",
    "                self.y_deep = tf.layers.dense(self.y_deep, self.config['deep_layers'][i], \n",
    "                                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                             bias_initializer=tf.initializers.constant(0.01),\n",
    "                                             activation=None)\n",
    "                \n",
    "                self.y_deep = tf.nn.relu(self.y_deep)\n",
    "                #self.y_deep = tf.nn.dropout(self.y_deep, self.dropout_keep_prob[i+1])\n",
    "                self.y_deep = tf.layers.batch_normalization(self.y_deep, training=self.is_training)\n",
    "        \n",
    "        with tf.name_scope(\"output\"):\n",
    "            '''\n",
    "            self.logits = tf.add(tf.matmul(self.y_deep, self.weights['output']), self.weights['output_bias'])\n",
    "            '''\n",
    "            self.logits = tf.layers.dense(self.y_deep, self.config['num_classes'],\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                        bias_initializer=tf.initializers.constant(0.01),\n",
    "                                        activation=None)\n",
    "            \n",
    "            self.predictions = tf.nn.sigmoid(self.logits)  # 得到概率\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n",
    "            #losses = tf.losses.log_loss(self.labels, self.predictions)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            # 计算l2正则化损失\n",
    "            if self.config['l2_reg'] > 0: \n",
    "                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) \n",
    "                                    for cand_var in tf.trainable_variables() \n",
    "                                    if \"bia\" not in cand_var.name and \"embedding\" not in cand_var.name])\n",
    "                self.l2_loss = l2_loss\n",
    "                self.loss += self.config['l2_reg'] * self.l2_loss\n",
    "            \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "\n",
    "            learning_rate = tf.train.exponential_decay(self.config['lr'],\n",
    "                                                      self.global_step_tensor,\n",
    "                                                      self.config['decay_steps'],\n",
    "                                                      self.config['decay_rate'],\n",
    "                                                      staircase=True)\n",
    "            # 使用梯度削减防止梯度爆炸\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            \n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "            \n",
    "            \n",
    "            for idx, (grad, var) in enumerate(grads_and_vars):\n",
    "                if grad is not None:\n",
    "                    grads_and_vars[idx] = (tf.clip_by_norm(grad, self.config['grad_clip']), var)\n",
    "            \n",
    "            self.train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step_tensor)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    # 初始化全部的权重\n",
    "    def _init_weights(self):\n",
    "        weights = dict()\n",
    "        \n",
    "        # embeddings\n",
    "        weights['feature_embeddings'] = tf.Variable(tf.truncated_normal([self.config['feature_size'], self.config['embedding_size']], \n",
    "                                                                     0.0, 0.01),\n",
    "                                                name='feature_embeddings')\n",
    "        weights['feature_bias'] = tf.Variable(tf.random_normal([self.config['feature_size'], 1], 0.0, 1.0), name='feature_bias')\n",
    "        \n",
    "        # Product Layer\n",
    "        if self.config['use_inner']:\n",
    "            weights['product_quadratic_inner'] = tf.Variable(tf.truncated_normal([self.config['deep_init_size'], self.config['field_size']], \n",
    "                                                                              0.0, 0.01),\n",
    "                                                            name=\"product_inner\")\n",
    "        else:\n",
    "            weights['product_quadratic_outer'] = tf.Variable(tf.truncated_normal([self.config['deep_init_size'], \n",
    "                                                                               self.config['embedding_size'], self.config['embedding_size']], \n",
    "                                                                              0.0, 0.01),\n",
    "                                                            name=\"product_outer\")\n",
    "        \n",
    "        weights['product_linear'] = tf.Variable(tf.truncated_normal([self.config['deep_init_size'], self.config['field_size'],\n",
    "                                                                 self.config['embedding_size']], 0.0, 0.01),\n",
    "                                               name=\"product_linear\")\n",
    "        weights['product_bias'] = tf.Variable(tf.truncated_normal([self.config['deep_init_size'],], 0.0, 1.0),\n",
    "                                             name=\"product_bias\")\n",
    "        \n",
    "        '''\n",
    "        # Deep Layers\n",
    "        num_layer = len(self.config['deep_layers'])\n",
    "        input_size = self.config['deep_init_size']\n",
    "        glorot = np.sqrt(2.0 / (input_size + self.config['deep_layers'][0]))\n",
    "        \n",
    "        weights['layer_0'] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(input_size, self.config['deep_layers'][0])),\n",
    "                                        dtype=tf.float32, name=\"layer_0\")\n",
    "        weights['bias_0'] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.config['deep_layers'][0])),\n",
    "                                       dtype=tf.float32, name=\"bias_0\")\n",
    "        \n",
    "        for i in range(1, num_layer):\n",
    "            glorot = np.sqrt(2.0 / (self.config['deep_layers'][i-1] + self.config['deep_layers'][i]))\n",
    "            weights[f\"layer_{i}\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, \n",
    "                                                                 size=(self.config['deep_layers'][i-1], self.config['deep_layers'][i])),\n",
    "                                               dtype=tf.float32, name=f\"layers_{i}\")\n",
    "            weights[f\"bias_{i}\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.config['deep_layers'][i])),\n",
    "                                              dtype=tf.float32, name=f\"bias_{i}\")\n",
    "            \n",
    "        glorot = np.sqrt(2.0 / (self.config['deep_layers'][-1] + 1))\n",
    "        weights['output'] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(self.config['deep_layers'][-1], self.config['num_classes'])),\n",
    "                                       dtype=tf.float32, name=\"output_weights\")\n",
    "        weights['output_bias'] = tf.Variable(tf.constant(0.01), dtype=tf.float32, name=\"output_bias\")\n",
    "        '''\n",
    "        return weights\n",
    "    \n",
    "    def init_saver(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=self.config['max_to_keep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(BaseTrain):\n",
    "    def __init__(self, sess, model, data, config, logger):\n",
    "        super().__init__(sess, model, data, config, logger)\n",
    "        self.train = data[0]\n",
    "        self.eval = data[1]\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        # 定义迭代次数\n",
    "        num_iter_per_epoch = self.train.length // self.config['batch_size']\n",
    "        \n",
    "        for _ in tqdm(range(num_iter_per_epoch)):\n",
    "            loss, metrics, step = self.train_step()\n",
    "            train_acc, train_f_score = metrics['accuracy'], metrics['f_score']\n",
    "            \n",
    "            ## 将训练过程的损失写入\n",
    "            summaries_dict = {\"loss\": loss, \n",
    "                             \"acc\": np.array(train_acc),\n",
    "                             \"f_score\": np.array(train_f_score)}\n",
    "            self.logger.summarize(step, summarizer='train', scope=\"train_summary\",\n",
    "                                 summaries_dict=summaries_dict)\n",
    "            \n",
    "            if step % self.config['evaluateEvery'] == 0:\n",
    "                print(\"Train - Step: {} | Loss: {} | Acc: {} | F1_score: {}\".format(\n",
    "                    step, loss, train_acc, train_f_score))\n",
    "                # 对测试集进行评估\n",
    "                eval_losses = []\n",
    "                eval_pred = []\n",
    "                eval_true = []\n",
    "                for batchEval in self.eval.iter_all(self.config['batch_size']):\n",
    "                    loss, predictions = self.eval_step(batchEval)\n",
    "                    eval_losses.append(loss)\n",
    "                    eval_pred.extend(predictions)\n",
    "                    eval_true.extend(batchEval[-1])\n",
    "                getMetric = Metric(np.array(eval_pred), np.array(eval_true), self.config)\n",
    "                metrics = getMetric.get_metrics()\n",
    "                acc_mean = np.round(metrics['accuracy'], 5)\n",
    "                gini_mean = np.round(metrics['gini_norm'], 5)\n",
    "                loss_mean = np.round(np.mean(eval_losses), 5)\n",
    "                print(\"Eval | Loss: {} | Accuracy: {} | Gini: {}\".format(\n",
    "                    loss_mean, acc_mean, gini_mean))\n",
    "                summaries_dict = {\"loss\": np.array(loss_mean),\n",
    "                                 \"accuracy\":np.array(acc_mean),\n",
    "                                 \"gini\": np.array(gini_mean)}\n",
    "                self.logger.summarize(step, summarizer=\"test\", scope=\"test_summary\",\n",
    "                                     summaries_dict=summaries_dict)\n",
    "            if step % self.config['checkpointEvery'] == 0:\n",
    "                self.model.save(self.sess)\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_feat_i, batch_feat_v, batch_y = next(self.train.next_batch(self.config['batch_size']))\n",
    "        feed_dict = {self.model.feat_index: batch_feat_i, \n",
    "                    self.model.feat_value: batch_feat_v, \n",
    "                    self.model.labels: batch_y,\n",
    "                    self.model.dropout_keep_prob: self.config['dropout_prob'],\n",
    "                    self.model.is_training: True}\n",
    "        _, loss, predictions, step, l2_loss = self.sess.run([self.model.train_op,\n",
    "                                                   self.model.loss, \n",
    "                                                   self.model.predictions,\n",
    "                                                   self.model.global_step_tensor,\n",
    "                                                   self.model.l2_loss],\n",
    "                                                  feed_dict=feed_dict)\n",
    "\n",
    "        if str(l2_loss) == \"nan\":\n",
    "            print(\"step: {} | loss: {} | l2_loss: {} \".format(step, loss, l2_loss))\n",
    "        getMetric = Metric(predictions, batch_y, self.config)\n",
    "        metrics = getMetric.get_metrics()\n",
    "        return loss, metrics, step \n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        feed_dict = {self.model.feat_index: batch[0],\n",
    "                    self.model.feat_value: batch[1],\n",
    "                    self.model.labels: batch[2],\n",
    "                    self.model.dropout_keep_prob: [1.0] * len(self.config['dropout_prob']),\n",
    "                    self.model.is_training: False}\n",
    "        loss, predictions = self.sess.run([self.model.loss, self.model.predictions],\n",
    "                                         feed_dict=feed_dict)\n",
    "        return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, labels, *features):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.length = len(labels)\n",
    "        ## 计算不同类别的比例\n",
    "        unique = Counter(self.labels.ravel())\n",
    "        self.ratio = [(key, value / self.length) for key, value in unique.items()]\n",
    "        self.indices = []\n",
    "        for key, _ in self.ratio:\n",
    "            index = np.where(labels.ravel() == key)\n",
    "            self.indices.append(index)\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        '''\n",
    "        生成每一个batch的数据集\n",
    "        '''\n",
    "        choose = np.array([])\n",
    "        for i in range(len(self.indices)):\n",
    "            ## 按照在数据集中出现的比例采样\n",
    "            idx = np.random.choice(self.indices[i][0],\n",
    "                                   max(1, min(len(self.indices[i][0]), int(batch_size * self.ratio[i][1]))))\n",
    "            '''\n",
    "            ## 等比例采样\n",
    "            idx = np.random.choice(self.indices[i][0],\n",
    "                                  min(len(self.indices[i][0]), int(batch_size / len(self.indices))))\n",
    "            '''\n",
    "            choose = np.append(choose, idx)\n",
    "        choose = np.random.permutation(choose).astype(\"int64\")\n",
    "        result = []\n",
    "        for feat in self.features:\n",
    "            result.append(feat[choose])\n",
    "        result.append(self.labels[choose])\n",
    "        yield result\n",
    "        \n",
    "    def iter_all(self, batch_size):\n",
    "        '''\n",
    "        按照batch迭代所有数据\n",
    "        '''\n",
    "        numBatches = self.length // batch_size + 1 \n",
    "        for i in range(numBatches):\n",
    "            result = []\n",
    "            start = i*batch_size\n",
    "            end = min(start+batch_size, self.length)\n",
    "            for feat in self.features:\n",
    "                result.append(np.asarray(feat[start:end]))\n",
    "            result.append(np.asarray(self.labels[start:end]))\n",
    "            yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_idx = slice(0, int(len(labels)*0.8))\n",
    "val_idx = slice(int(len(labels)*0.8), int(len(labels)))\n",
    "\n",
    "train_df_i, train_df_v, train_df_y = (features[\"df_i\"][train_idx], \n",
    "                                      features[\"df_v\"][train_idx], \n",
    "                                      labels[train_idx])\n",
    "val_df_i, val_df_v, val_df_y = (features[\"df_i\"][val_idx],\n",
    "                               features[\"df_v\"][val_idx],\n",
    "                               labels[val_idx])\n",
    "\n",
    "train = DataGenerator(train_df_y, train_df_i, train_df_v)\n",
    "val = DataGenerator(val_df_y, val_df_i, val_df_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(field_handler)\n",
    "    config['num_epochs'] = 2 \n",
    "    create_dirs([config['summary_dir'], config['checkpoint_dir']])\n",
    "    tf.reset_default_graph()\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    \n",
    "    model = PNN(config)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    pack_data = [train, val]\n",
    "    logger = Logger(sess, config)\n",
    "    trainer = Trainer(sess, model, pack_data, config, logger)\n",
    "    trainer.train_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "当前正处于第1次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838deb90f3544b19984bed68991753c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 1000 | Loss: 0.4938005208969116 | Acc: 0.81102 | F1_score: 0.45455\n",
      "Eval | Loss: 0.6075699925422668 | Accuracy: 0.74884 | Gini: 0.48812\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 2000 | Loss: 0.466257780790329 | Acc: 0.77953 | F1_score: 0.44\n",
      "Eval | Loss: 0.5628700256347656 | Accuracy: 0.74884 | Gini: 0.50448\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 3000 | Loss: 0.4092872738838196 | Acc: 0.7874 | F1_score: 0.49057\n",
      "Eval | Loss: 0.5942999720573425 | Accuracy: 0.74882 | Gini: 0.50261\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 4000 | Loss: 0.46430566906929016 | Acc: 0.8189 | F1_score: 0.56604\n",
      "Eval | Loss: 0.6249200105667114 | Accuracy: 0.74884 | Gini: 0.51305\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 5000 | Loss: 0.5512120723724365 | Acc: 0.77953 | F1_score: 0.48148\n",
      "Eval | Loss: 0.5398300290107727 | Accuracy: 0.7535 | Gini: 0.51335\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 6000 | Loss: 0.4614016115665436 | Acc: 0.75591 | F1_score: 0.39216\n",
      "Eval | Loss: 0.5640100240707397 | Accuracy: 0.74899 | Gini: 0.51116\n",
      "Saving model...\n",
      "WARNING:tensorflow:From /home/chen/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Model saved\n",
      "\n",
      "当前正处于第2次迭代\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9ac0c0fd2d4f14a036ae5ba93a6206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Step: 7000 | Loss: 0.37560948729515076 | Acc: 0.85827 | F1_score: 0.67857\n",
      "Eval | Loss: 0.5682600140571594 | Accuracy: 0.74882 | Gini: 0.50216\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 8000 | Loss: 0.47730201482772827 | Acc: 0.7874 | F1_score: 0.49057\n",
      "Eval | Loss: 0.5453500151634216 | Accuracy: 0.75174 | Gini: 0.50011\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 9000 | Loss: 0.4802996814250946 | Acc: 0.82677 | F1_score: 0.56\n",
      "Eval | Loss: 0.5201399922370911 | Accuracy: 0.75908 | Gini: 0.49285\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 10000 | Loss: 0.45368653535842896 | Acc: 0.80315 | F1_score: 0.54545\n",
      "Eval | Loss: 0.5383599996566772 | Accuracy: 0.75314 | Gini: 0.50078\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 11000 | Loss: 0.3806925117969513 | Acc: 0.84252 | F1_score: 0.64286\n",
      "Eval | Loss: 0.5150799751281738 | Accuracy: 0.75926 | Gini: 0.50125\n",
      "Saving model...\n",
      "Model saved\n",
      "Train - Step: 12000 | Loss: 0.40783533453941345 | Acc: 0.81102 | F1_score: 0.53846\n",
      "Eval | Loss: 0.4935399889945984 | Accuracy: 0.77139 | Gini: 0.50459\n",
      "Saving model...\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用Inner的方法存在梯度消失和梯度爆炸的情况，导致参数为nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
